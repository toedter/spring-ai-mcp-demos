server.port: 8083

spring:
  ai:
    mcp:
      client:
        enabled: true
        name: demo-mcp-server
        version: 0.0.1
        type: SYNC
        stdio:
          enabled: true
          servers-configuration: classpath:mcp_servers_config.json
        toolcallback:
          enabled: true
#    ollama:
#      base-url: http://localhost:11434
#      chat:
#        options:
#          model: qwen3:8b
#          temperature: 0.7
    openai:
      base-url: https://api.siemens.com/llm
      chat:
        options:
          model: qwen3-30b-a3b
          temperature: 0.7
      api-key: ${SIEMENS_LLM_API_KEY}
logging:
  level:
    root: INFO
